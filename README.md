# CIM-Accelerator-Compiler-based-on-TVM
*This is a project named CIM-Accelerator-Compiler-based-on-TVM.*

---
## About the compiler and the files
**The compiler**
1. TVM-based
We use TVM because it has an unified IR for all kinds of Network structure input. And in the compiler example,
the input definition of the network is described in python with pytorch. Also, TVM has BYOC(Bring your own codegen),
which can help us easily fuse the ops into the pattern our device supports and generate a JSON format.

2. CIM-Accelerator-Compiler
With the JSON format generated by TVM, we analysis the JSON node and use our algorithm to do the mapping and placement
work. Our CIM accelerator supports only a few kinds of op nodes, which have the similar pattern. With the nodes pattern,
we analysis the storage of the weights and the activations, then map and place the data on 1 muyan(our CIM accelerator).
Finally, we generate an inst file which can work on both FPGA and the real chip with the compiler.

**The files**
1. bias_scale
In this file, there are the bias and the scale file with the correct format. And now there are the corresponding files 
for our example,parts of a modified tiny yolo_v3 model. Also we provide the generate file if you want to generate your 
own bias/scale file.

2. cim
In this file, there are the main algorithm of compiling. `knight` defines the device's inst parser. `model` is for saving
the input network model. `src` includes the algorithm.  

3. result
The intermediate and final result will be saved in this file, in case you may want to check the proccess. Of course, the 
final result `test_inst.txt` will also be saved here if you run the `run_case.py`.

4. tvm
Here is the modified tvm.

## How to work(To be done)
1. Set up with TVM first

2. Complete the info

3. Run the case file

4. To check the result and use the inst in simulation